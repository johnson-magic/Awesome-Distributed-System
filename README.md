# Awesome-Distributed-System

## 数据并行
- [ ] [Parameter Server for Distributed Machine Learning/Ps](https://www.cs.cmu.edu/~muli/file/ps.pdf)

- [ ] [Implementing Remote Procedure Calls/RPC](https://web.eecs.umich.edu/~mosharaf/Readings/RPC.pdf)

- [ ] [Horovod: fast and easy distributed deep learning in TensorFlow/Horovod](https://arxiv.org/pdf/1802.05799.pdf)

- [ ] [Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour](https://arxiv.org/pdf/1706.02677.pdf)

- [ ] [ImageNet Training in Minutes](https://arxiv.org/pdf/1709.05011.pdf)
    
- [ ] [Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes](https://arxiv.org/pdf/1807.11205.pdf)

- [ ] [Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes](https://arxiv.org/pdf/1902.06855.pdf?source=post_page---------------------------)

- [ ] [Heterogeneity-Aware Asynchronous Decentralized Training] (ai.intsig.net/2051/magic_zhang/explorer.html)

- [ ] [Taming Unbalanced Training Workloads in Deep Learning with Partial Collective Operations](https://arxiv.org/pdf/1908.04207.pdf)

- [ ] [SwarmSGD: Scalable Decentralized SGD with Local Updates](https://parsa.epfl.ch/course-info/cs723/papers/swarmsgd.pdf)

- [ ] [A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters](https://www.usenix.org/system/files/osdi20-jiang.pdf)

- [ ] [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/pdf/2101.06840.pdf)

- [ ] [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/pdf/2104.07857.pdf) 
