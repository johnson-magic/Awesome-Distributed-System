# Awesome-Distributed-System

## 数据并行
- [ ] [Parameter Server for Distributed Machine Learning/Ps](https://www.cs.cmu.edu/~muli/file/ps.pdf)

- [ ] [Implementing Remote Procedure Calls/RPC](https://web.eecs.umich.edu/~mosharaf/Readings/RPC.pdf)

- [ ] [Horovod: fast and easy distributed deep learning in TensorFlow/Horovod](https://arxiv.org/pdf/1802.05799.pdf)

- [ ] [Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour](https://arxiv.org/pdf/1706.02677.pdf)

- [ ] [ImageNet Training in Minutes](https://arxiv.org/pdf/1709.05011.pdf)
    
- [ ] [Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes](https://arxiv.org/pdf/1807.11205.pdf)

- [ ] [Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes](https://arxiv.org/pdf/1902.06855.pdf?source=post_page---------------------------)

- [ ] [Heterogeneity-Aware Asynchronous Decentralized Training](https://ai.intsig.net/2051/magic_zhang/explorer.html)

- [ ] [Taming Unbalanced Training Workloads in Deep Learning with Partial Collective Operations](https://arxiv.org/pdf/1908.04207.pdf)

- [ ] [SwarmSGD: Scalable Decentralized SGD with Local Updates](https://parsa.epfl.ch/course-info/cs723/papers/swarmsgd.pdf)

- [ ] [A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters](https://www.usenix.org/system/files/osdi20-jiang.pdf)

- [ ] [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/pdf/2101.06840.pdf)

- [ ] [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/pdf/2104.07857.pdf)

- [ ] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Model](https://www.cs.cmu.edu/~zhihaoj2/15-849/slides/13-zero-redundancy.pdf)

## 模型并行

- [ ] [One weird trick for parallelizing convolutional neural networks](https://arxiv.org/pdf/1404.5997.pdf)
- [ ] [Beyond Data and Model Parallelism for Deep Neural Networks](https://arxiv.org/pdf/1807.05358.pdf)
- [ ] [Supporting Very Large Models using Automatic Dataflow Graph Partitioning](https://arxiv.org/pdf/1807.08887.pdf)
- [ ] [Mesh-TensorFlow: Deep Learning for Supercomputers](https://arxiv.org/pdf/1811.02084.pdf)
- [ ] [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668.pdf)
- [ ] [GSPMD: General and Scalable Parallelization for ML Computation Graphs](https://arxiv.org/pdf/2105.04663.pdf)
- [ ] [*万亿 Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)
- [ ] [Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training](https://arxiv.org/pdf/2110.14883.pdf)
- [ ] [An Efficient 2D Method for Training Super-Large Deep Learning Models](https://arxiv.org/pdf/2104.05343.pdf)
- [ ] [Tesseract: Parallelize the Tensor Parallelism Efficiently](https://arxiv.org/pdf/2105.14500.pdf)
- [ ] [Maximizing Parallelism in Distributed Training for Huge Neural Networks](https://arxiv.org/pdf/2105.14450.pdf)

## pipe并行
- [ ] [Gpipe: Efficient training of giant neural networks using pipeline parallelism](https://arxiv.org/pdf/1811.06965.pdf)
- [ ] [Memory-Efficient Pipeline-Parallel DNN Training](https://arxiv.org/pdf/2006.09503.pdf)
- [ ] [PipeMare: Asynchronous Pipeline Parallel DNN Training](https://arxiv.org/pdf/1910.05124.pdf)
- [ ] [TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models](https://arxiv.org/pdf/2102.07988.pdf)
- [ ] [Maximizing Parallelism in Distributed Training for Huge Neural Networks](https://arxiv.org/pdf/2105.14450.pdf)

## 专家并行
- [ ] [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668.pdf)
- [ ] [GSPMD: General and Scalable Parallelization for ML Computation Graphs](https://arxiv.org/pdf/2105.04663.pdf)
- [ ] [BASE Layers: Simplifying Training of Large, Sparse Models](https://arxiv.org/pdf/2103.16716.pdf)
- [ ] [FASTMOE: A FAST MIXTURE-OF-EXPERT TRAINING SYSTEM](https://arxiv.org/pdf/2103.13262.pdf)

